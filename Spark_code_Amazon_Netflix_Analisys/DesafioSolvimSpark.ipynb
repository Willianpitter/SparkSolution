{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Spark\n",
    "conf = pyspark.SparkConf().setAppName('5GAnalisys').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute if the csv with all Netflix reviews is not available\n",
    "def make_netflix_csv(): # Read all txt file and store them in one big file\n",
    "    files = ['./netflix_reviews/combined_data_1.txt','./netflix_reviews/combined_data_2.txt','./netflix_reviews/combined_data_3.txt',\n",
    "            './netflix_reviews/combined_data_4.txt']\n",
    "    if not os.path.isfile('netflix_reviews.csv'):\n",
    "        data = open('netflix_reviews.csv', mode='w')\n",
    "\n",
    "        row = list()\n",
    "        for file in files:\n",
    "            print('reading ratings from {}...'.format(file))\n",
    "            with open(file) as f:\n",
    "                for line in f:\n",
    "                    del row[:]\n",
    "                    line = line.strip()\n",
    "                    if line.endswith(':'):\n",
    "                        #all are rating\n",
    "                        movid_id = line.replace(':', '')\n",
    "                    else:\n",
    "                        row = [x for x in line.split(',')]\n",
    "                        row.insert(0, movid_id)\n",
    "                        data.write(','.join(row))\n",
    "                        data.write('\\n')\n",
    "            print('Done.\\n')\n",
    "        data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the dataframe from data.csv file..\n"
     ]
    }
   ],
   "source": [
    "# Defining paths to files\n",
    "path_netflix_reviews = 'netflix_reviews.csv'\n",
    "path_netflix_movie_titles = 'netflix_reviews/movie_titles.csv'\n",
    "print('creating the dataframe from data.csv file..') # Criando os dataframes\n",
    "netflix_reviews = spark.read.csv(path_netflix_reviews).toDF('movie_id','user','rating','date')\n",
    "netflix_movie_titles = spark.read.csv(\"netflix_reviews/movie_titles.csv\",encoding = 'ISO-8859-1').toDF('movie_id', 'year_of_release', 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the dataframe from data.csv file..\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining paths to files\n",
    "path_amazon_digital_video = './amazon_reviews_us_Digital_Video_Download_v1_00/amazon_reviews_us_Digital_Video_Download_v1_00.tsv'\n",
    "print('creating the dataframe from data.csv file..') # Criando os data frames\n",
    "separator = '\\t'\n",
    "amazon_reviews = spark.read.csv(path_amazon_digital_video, sep = separator,header = True).toDF('marketplace', 'customer_id', 'review_id', 'product_id','product_parent', 'product_title', 'product_category', 'star_rating','helpful_votes', 'total_votes', 'vine', 'verified_purchase','review_headline', 'review_body', 'review_date')\n",
    "print('Done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_netflix_data(netflix_reviews): # IN: Netflix reviews Dataframe. OUT: Netflix reviews transformed.\n",
    "    netflix_reviews = netflix_reviews.join(netflix_movie_titles,netflix_movie_titles.movie_id == netflix_reviews.movie_id)\n",
    "    netflix_reviews = netflix_reviews.toDF('product_id','user','star_rating','date','movie_id_2','year_of_release','product_title')\n",
    "    netflix_reviews = netflix_reviews.drop(*['user','movie_id_2','date'])\n",
    "    netflix_reviews = netflix_reviews.withColumn('company_review',lit('netflix'))\n",
    "    return netflix_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_amazon_data(amazon_reviews): #IN: Amazon reviews dataframe. OUT: Amazon dataframe transformed.\n",
    "    amazon_reviews = amazon_reviews.withColumn('company_review',lit('amazon'))\n",
    "    amazon_reviews = amazon_reviews.drop(*['marketplace','customer_id','review_id','product_parent','review_date',\n",
    "                                      'product_category','helpful_votes','total_votes','vine','verified_purchase',\n",
    "                                      'review_headline','review_body'])\n",
    "    amazon_reviews = amazon_reviews.withColumn('year_of_release',lit(None))\n",
    "    return amazon_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+---------------+--------------+\n",
      "|product_id|star_rating|year_of_release|  product_title|company_review|\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "|         1|          3|           2003|Dinosaur Planet|       netflix|\n",
      "|         1|          5|           2003|Dinosaur Planet|       netflix|\n",
      "|         1|          4|           2003|Dinosaur Planet|       netflix|\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "netflix_reviews_transformed = transform_netflix_data(netflix_reviews)\n",
    "netflix_reviews_transformed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+--------------+---------------+\n",
      "|product_id|       product_title|star_rating|company_review|year_of_release|\n",
      "+----------+--------------------+-----------+--------------+---------------+\n",
      "|B00AYB1482|Enlightened: Seas...|          5|        amazon|           null|\n",
      "|B00KQD28OM|             Vicious|          5|        amazon|           null|\n",
      "|B01489L5LQ|         After Words|          4|        amazon|           null|\n",
      "+----------+--------------------+-----------+--------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_reviews_transformed = transform_amazon_data(amazon_reviews)\n",
    "amazon_reviews_transformed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_dataframes(netflix_reviews_transformed,amazon_reviews_transformed): #IN: Netflix and Amazon reviews transformed. OUT: Joined dataframe.\n",
    "    all_reviews = netflix_reviews_transformed.union(amazon_reviews_transformed.select('product_id','star_rating','year_of_release','product_title','company_review'))\n",
    "    all_reviews = all_reviews.withColumn(\"star_rating\", col(\"star_rating\").cast(\"int\"))\n",
    "    all_reviews = all_reviews.withColumn(\"year_of_release\", col(\"year_of_release\").cast(\"int\"))\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------------+---------------+--------------+\n",
      "|product_id|star_rating|year_of_release|  product_title|company_review|\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "|         1|          3|           2003|Dinosaur Planet|       netflix|\n",
      "|         1|          5|           2003|Dinosaur Planet|       netflix|\n",
      "|         1|          4|           2003|Dinosaur Planet|       netflix|\n",
      "|         1|          4|           2003|Dinosaur Planet|       netflix|\n",
      "|         1|          3|           2003|Dinosaur Planet|       netflix|\n",
      "+----------+-----------+---------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_reviews = union_dataframes(netflix_reviews_transformed,amazon_reviews_transformed)\n",
    "all_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This csv file already exists\n",
      "This parquet file already exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    all_reviews.write.csv(\"all_reviews.csv\")\n",
    "except:\n",
    "    print(\"This csv file already exists\")\n",
    "try:\n",
    "    all_reviews.write.parquet(\"all_reviews.parquet\")\n",
    "except:\n",
    "    print(\"This parquet file already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queryes que respondem as perguntas de negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantos filmes estão disponíveis na Amazon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.createOrReplaceTempView(\"all_reviews\") # To use SQL statemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A amazon dispõe de 166748 filmes.\n"
     ]
    }
   ],
   "source": [
    "number_amazon_number = spark.sql(\"SELECT count(distinct product_id) from all_reviews WHERE company_review = 'amazon'\").collect()[0][0]\n",
    "print('A amazon dispõe de ' + str(number_amazon_number) + ' filmes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantos filmes estão disponíveis na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6898adad15d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnumber_netflix_movie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT count(distinct product_id) from all_reviews WHERE company_review = 'netflix'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A netflix dispõe de '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_netflix_movie\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' filmes.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PysparkProject\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PysparkProject\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1303\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m~\\Documents\\PysparkProject\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PysparkProject\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_netflix_movie = spark.sql(\"SELECT count(distinct product_id) from all_reviews WHERE company_review = 'netflix'\").collect()[0][0]\n",
    "print('A netflix dispõe de ' + str(number_netflix_movie) + ' filmes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dos filmes disponíveis na Amazon, quantos % estão disponíveis na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_amazon_movies_in_netflix = spark.sql(\"SELECT count(distinct product_title) from all_reviews t1 WHERE company_review='amazon' and EXISTS(SELECT product_title from all_reviews t2 where LOWER(t1.product_title)=LOWER(t2.product_title) and t2.company_review!=t1.company_review)\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = (number_amazon_movies_in_netflix[0][0] / number_amazon_number)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dos filmes disponíveis na Amazon, '+ str(round(percentage,2))+'% estão disponíveis na Netflix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O quão perto a médias das notas dos filmes disponíveis na Amazon está dos filmes disponíveis na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_avg = spark.sql(\"SELECT avg(star_rating) from all_reviews WHERE company_review == 'netflix'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_avg = spark.sql(\"SELECT avg(star_rating) from all_reviews WHERE company_review == 'amazon'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A média de notas dos filmes disponíveis na Amazon é ' + str(round(amazon_avg[0][0],3)) + ', e a média de notas dos filmes da Netflix é '+ str(round(netflix_avg[0][0],3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual ano de lançamento possui mais filmes na Amazon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não há este dado disponível no dataset disponibilizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual ano de lançamento possui mais filmes na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_max = spark.sql(\"SELECT year_of_release, count(year_of_release) from all_reviews GROUP BY year_of_release ORDER BY count(year_of_release) desc limit 1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('O ano com mais lançamentos na netflix é ' + str(year_max[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quais filmes que não estão disponíveis no catálogo da Netflix foram melhor avaliados (notas 4 e 5)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_movies_not_in_netflix = spark.sql(\"SELECT * from (select distinct(product_title) from all_reviews t1 WHERE company_review='amazon' and NOT EXISTS(select product_title from all_reviews t2 where LOWER(t1.product_title)=LOWER(t2.product_title) and t2.company_review!=t1.company_review and t1.company_review = 'amazon'))\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(best_movies_not_in_netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_movies_not_in_netflix.txt\", 'w',encoding=\"utf-8\") as file:\n",
    "        for element in best_movies_not_in_netflix:\n",
    "            file.write(str(list(element))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quais filmes que não estão disponíveis no catálogo da Amazon foram melhor avaliados (notas 4 e 5)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_movies_not_in_amazon = spark.sql(\"SELECT * from (SELECT distinct(product_title) from all_reviews t1 WHERE company_review='netflix' and star_rating >= 4 and NOT EXISTS(SELECT product_title from all_reviews t2 WHERE LOWER(t1.product_title)=LOWER(t2.product_title) and t2.company_review!=t1.company_review and t1.company_review = 'netflix'))\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(best_movies_not_in_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_movies_not_in_amazon.txt\", 'w',encoding=\"utf-8\") as file:\n",
    "        for element in best_movies_not_in_amazon:\n",
    "            file.write(str(list(element))+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
