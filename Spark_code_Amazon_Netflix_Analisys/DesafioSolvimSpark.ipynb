{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment findspark module if you are not using in Windows\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "# \n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=5GAnalisys, master=local) created by __init__ at <ipython-input-4-7cd696b378fa>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7cd696b378fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Config Spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'5GAnalisys'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PysparkProject\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\Documents\\PysparkProject\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                     \u001b[1;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    343\u001b[0m                         \u001b[1;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m                         \u001b[1;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=5GAnalisys, master=local) created by __init__ at <ipython-input-4-7cd696b378fa>:3 "
     ]
    }
   ],
   "source": [
    "# Config Spark\n",
    "conf = pyspark.SparkConf().setAppName('5GAnalisys').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute if the csv with all Netflix reviews is not available\n",
    "def make_netflix_csv(): # Read all txt file and store them in one big file\n",
    "    files = ['./netflix_reviews/combined_data_1.txt','./netflix_reviews/combined_data_2.txt','./netflix_reviews/combined_data_3.txt',\n",
    "            './netflix_reviews/combined_data_4.txt']\n",
    "    \n",
    "    \n",
    "    if not os.path.isfile('./netflix_reviews.csv'):\n",
    "        data = open('netflix_reviews.csv', mode='w')\n",
    "\n",
    "        row = list()\n",
    "        for file in files:\n",
    "            print('reading ratings from {}...'.format(file))\n",
    "            with open(file) as f:\n",
    "                for line in f:\n",
    "                    del row[:]\n",
    "                    line = line.strip()\n",
    "                    if line.endswith(':'):\n",
    "                        #all are rating\n",
    "                        movid_id = line.replace(':', '')\n",
    "                    else:\n",
    "                        row = [x for x in line.split(',')]\n",
    "                        row.insert(0, movid_id)\n",
    "                        data.write(','.join(row))\n",
    "                        data.write('\\n')\n",
    "            print('Done.\\n')\n",
    "        data.close() \n",
    "    else:\n",
    "        print('The CSV already exists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV already exists\n"
     ]
    }
   ],
   "source": [
    "make_netflix_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining paths to files\n",
    "path_netflix_reviews = 'netflix_reviews.csv'\n",
    "path_netflix_movie_titles = 'netflix_reviews/movie_titles.csv'\n",
    "if os.path.getsize(path_netflix_reviews) > 0:\n",
    "    print('creating the dataframe from netflix_reviews.csv file..') # Criando os dataframes\n",
    "    spark.read.csv(path_netflix_reviews).show()\n",
    "    netflix_reviews = spark.read.csv(path_netflix_reviews).toDF('movie_id_2','user','rating','date')\n",
    "    netflix_movie_titles = spark.read.csv(\"netflix_reviews/movie_titles.csv\",encoding = 'ISO-8859-1').toDF('movie_id', 'year_of_release', 'title')\n",
    "else:\n",
    "    print(\"The netflix_reviews.csv file is empty, please delete the empty file netflix_reviews.csv and run the function make_netflix_csv() again to create a valid CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining paths to files\n",
    "path_amazon_digital_video = './amazon_reviews_us_Digital_Video_Download_v1_00/amazon_reviews_us_Digital_Video_Download_v1_00.tsv'\n",
    "print('creating the dataframe from data.csv file..') # Criando os data frames\n",
    "separator = '\\t'\n",
    "amazon_reviews = spark.read.csv(path_amazon_digital_video, sep = separator,header = True).toDF('marketplace', 'customer_id', 'review_id', 'product_id','product_parent', 'product_title', 'product_category', 'star_rating','helpful_votes', 'total_votes', 'vine', 'verified_purchase','review_headline', 'review_body', 'review_date')\n",
    "print('Done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_netflix_data(netflix_reviews): # IN: Netflix reviews Dataframe. OUT: Netflix reviews transformed.\n",
    "    netflix_reviews = netflix_reviews.join(netflix_movie_titles,netflix_movie_titles.movie_id == netflix_reviews.movie_id_2)\n",
    "    netflix_reviews = netflix_reviews.toDF('product_id','user','star_rating','date','movie_id_2','year_of_release','product_title')\n",
    "    netflix_reviews = netflix_reviews.drop(*['user','movie_id_2','date'])\n",
    "    netflix_reviews = netflix_reviews.withColumn('company_review',lit('netflix'))\n",
    "    return netflix_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_amazon_data(amazon_reviews): #IN: Amazon reviews dataframe. OUT: Amazon dataframe transformed.\n",
    "    amazon_reviews = amazon_reviews.withColumn('company_review',lit('amazon'))\n",
    "    amazon_reviews = amazon_reviews.drop(*['marketplace','customer_id','review_id','product_parent','review_date',\n",
    "                                      'product_category','helpful_votes','total_votes','vine','verified_purchase',\n",
    "                                      'review_headline','review_body'])\n",
    "    amazon_reviews = amazon_reviews.withColumn('year_of_release',lit(None))\n",
    "    return amazon_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_reviews_transformed = transform_netflix_data(netflix_reviews)\n",
    "netflix_reviews_transformed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_transformed = transform_amazon_data(amazon_reviews)\n",
    "amazon_reviews_transformed.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_dataframes(netflix_reviews_transformed,amazon_reviews_transformed): #IN: Netflix and Amazon reviews transformed. OUT: Joined dataframe.\n",
    "    all_reviews = netflix_reviews_transformed.union(amazon_reviews_transformed.select('product_id','star_rating','year_of_release','product_title','company_review'))\n",
    "    all_reviews = all_reviews.withColumn(\"star_rating\", col(\"star_rating\").cast(\"int\"))\n",
    "    all_reviews = all_reviews.withColumn(\"year_of_release\", col(\"year_of_release\").cast(\"int\"))\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = union_dataframes(netflix_reviews_transformed,amazon_reviews_transformed)\n",
    "all_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    all_reviews.write.csv(\"all_reviews.csv\")\n",
    "except:\n",
    "    print(\"This csv file already exists\")\n",
    "try:\n",
    "    all_reviews.write.parquet(\"all_reviews.parquet\")\n",
    "except:\n",
    "    print(\"This parquet file already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queryes que respondem as perguntas de negócio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantos filmes estão disponíveis na Amazon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.createOrReplaceTempView(\"all_reviews\") # To use SQL statemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A amazon dispõe de 166748 filmes.\n"
     ]
    }
   ],
   "source": [
    "number_amazon_number = spark.sql(\"SELECT count(distinct product_id) from all_reviews WHERE company_review = 'amazon'\").collect()[0][0]\n",
    "print('A amazon dispõe de ' + str(number_amazon_number) + ' filmes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantos filmes estão disponíveis na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A netflix dispõe de 17770 filmes.\n"
     ]
    }
   ],
   "source": [
    "number_netflix_movie = spark.sql(\"SELECT count(distinct product_id) from all_reviews WHERE company_review = 'netflix'\").collect()[0][0]\n",
    "print('A netflix dispõe de ' + str(number_netflix_movie) + ' filmes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dos filmes disponíveis na Amazon, quantos % estão disponíveis na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_amazon_movies_in_netflix = spark.sql(\"SELECT count(distinct product_title) from all_reviews t1 WHERE company_review='amazon' and EXISTS(SELECT product_title from all_reviews t2 where LOWER(t1.product_title)=LOWER(t2.product_title) and t2.company_review!=t1.company_review)\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = (number_amazon_movies_in_netflix[0][0] / number_amazon_number)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dos filmes disponíveis na Amazon, 3.9% estão disponíveis na Netflix\n"
     ]
    }
   ],
   "source": [
    "print('Dos filmes disponíveis na Amazon, '+ str(round(percentage,2))+'% estão disponíveis na Netflix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O quão perto a médias das notas dos filmes disponíveis na Amazon está dos filmes disponíveis na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_avg = spark.sql(\"SELECT avg(star_rating) from all_reviews WHERE company_review == 'netflix'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_avg = spark.sql(\"SELECT avg(star_rating) from all_reviews WHERE company_review == 'amazon'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A média de notas dos filmes disponíveis na Amazon é 4.201, e a média de notas dos filmes da Netflix é 3.604\n"
     ]
    }
   ],
   "source": [
    "print('A média de notas dos filmes disponíveis na Amazon é ' + str(round(amazon_avg[0][0],3)) + ', e a média de notas dos filmes da Netflix é '+ str(round(netflix_avg[0][0],3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual ano de lançamento possui mais filmes na Amazon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não há este dado disponível no dataset disponibilizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual ano de lançamento possui mais filmes na Netflix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_max = spark.sql(\"SELECT year_of_release, count(year_of_release) from all_reviews GROUP BY year_of_release ORDER BY count(year_of_release) desc limit 1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O ano com mais lançamentos na netflix é 2004\n"
     ]
    }
   ],
   "source": [
    "print('O ano com mais lançamentos na netflix é ' + str(year_max[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quais filmes que não estão disponíveis no catálogo da Netflix foram melhor avaliados (notas 4 e 5)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_movies_not_in_netflix = spark.sql(\"SELECT * from (select distinct(product_title) from all_reviews t1 WHERE company_review='amazon' and NOT EXISTS(select product_title from all_reviews t2 where LOWER(t1.product_title)=LOWER(t2.product_title) and t2.company_review!=t1.company_review and t1.company_review = 'amazon'))\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84467"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_movies_not_in_netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_movies_not_in_netflix.txt\", 'w',encoding=\"utf-8\") as file:\n",
    "        for element in best_movies_not_in_netflix:\n",
    "            file.write(str(list(element))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quais filmes que não estão disponíveis no catálogo da Amazon foram melhor avaliados (notas 4 e 5)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "    best_movies_not_in_amazon = spark.sql(\"SELECT * from (SELECT distinct(product_title) from all_reviews t1 WHERE company_review='netflix' and star_rating >= 4 and NOT EXISTS(SELECT product_title from all_reviews t2 WHERE LOWER(t1.product_title)=LOWER(t2.product_title) and t2.company_review!=t1.company_review and t1.company_review = 'netflix'))\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10951"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_movies_not_in_amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_movies_not_in_amazon.txt\", 'w',encoding=\"utf-8\") as file:\n",
    "        for element in best_movies_not_in_amazon:\n",
    "            file.write(str(list(element))+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
